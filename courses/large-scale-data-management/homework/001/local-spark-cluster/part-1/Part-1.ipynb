{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUEB M.Sc. in Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Course: Large Scale Data Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submission of <mark>Spiros Politis (p3351814)</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from operator import add\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Strips a line of text from whitespace, \n",
    "    removes punctuation and convert to lowercase.\n",
    "'''\n",
    "def cleanup_text(text):\n",
    "    import string\n",
    "    \n",
    "    # Strip the line of text from leading and trailing white space\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    \n",
    "    # Convert all words to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "'''\n",
    "    Generator function that yields a mapper key.\n",
    "'''\n",
    "def get_key(line):\n",
    "    # Split line into words\n",
    "    words = line.split()\n",
    "\n",
    "    # Counter demarkating tuple bounds (x, x + 1), x = current word\n",
    "    i = 0\n",
    "    \n",
    "    while(i < len(words) - 1):\n",
    "        # Produce a key with format\n",
    "        # <word at pos x>-<word at pos x + 1>\n",
    "        # Tab (\\t) demarkates key, value\n",
    "        yield words[i] + \",\" + words[i + 1]\n",
    "        \n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get or create Spark context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_context = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read text file from HDFS and get a RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_rdd = spark_context.textFile(\"hdfs://namenode/data/shakespeare.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Spark job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup text\n",
    "# Get key\n",
    "# Map each key to value 1 \n",
    "# Reduce by key with add operation\n",
    "word_count = text_rdd \\\n",
    "    .map(lambda line: cleanup_text(line)) \\\n",
    "    .flatMap(lambda line: get_key(line)) \\\n",
    "    .map(lambda line: (line, 1)) \\\n",
    "    .reduceByKey(add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save job output to HDFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count.coalesce(1).saveAsTextFile(\"hdfs://namenode/data/part-1/spark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, print output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_collect = word_count.collect()\n",
    "\n",
    "for (word, count) in word_count_collect:\n",
    "    print(\"{}\\t{}\".format(word, count))\n",
    "    \n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
